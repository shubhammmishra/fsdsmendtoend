{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-07 10:28:01.239649\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILE=f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\DEELL\\\\Desktop\\\\fsdsmendtoend\\\\notebook'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\DEELL\\\\Desktop\\\\fsdsmendtoend\\\\notebook\\\\logs'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(),\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DimondPricePridiction.logger import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(' i have just tested the things')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\DEELL\\\\Desktop\\\\fsdsmendtoend\\\\notebook\\\\data\\\\gemstone.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(),\"data\\\\gemstone.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#data=  Path(\"notebook\\\\data\",\"gemstone.csv\")\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#os.path.join(\"notebook\\\\data\",\"gemstone.csv\")\u001b[39;00m\n\u001b[0;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclarity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "data= Path(os.path.join(os.getcwd(),\"data\\gemstone.csv\"))\n",
    "#data=  Path(\"notebook\\\\data\",\"gemstone.csv\")\n",
    "#os.path.join(\"notebook\\\\data\",\"gemstone.csv\")\n",
    "data = pd.read_csv(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.DimondPricePridiction.logger import logging\n",
    "from src.DimondPricePridiction.exception import customexception\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "class DataIngestionConfig:\n",
    "    raw_data_path:str= os.path.join(\"artifacts\",\"raw.csv\")\n",
    "    train_data_path:str= os.path.join(\"artifacts\",\"train.csv\")\n",
    "    test_data_path:str= os.path.join(\"artifacts\",\"test.csv\") \n",
    "\n",
    "\n",
    "\n",
    "class DataIngestion:\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ingestion_config=DataIngestionConfig()\n",
    "\n",
    "    def initiate_data_ingestion(self):\n",
    "        logging.info(\"data ingestion started\")\n",
    "\n",
    "        try:\n",
    "            data= str(Path(os.path.join(os.getcwd(),\"data\\gemstone.csv\")))\n",
    "            #data= pd.read_csv(Path(os.path.join(\"notebook/data\",\"gemstone.csv\")))\n",
    "            #data= pd.read_csv(os.path.join(os.getcwd(),\"data\\\\gemstone.csv\"))\n",
    "            logging.info(\" i have read dataset as a df\")\n",
    "            \n",
    "            os.makedirs(os.path.join(self.ingestion_config.raw_data_path),exist_ok= True)\n",
    "            data.to_csv(self.ingestion_config.raw_data_path, index=False)\n",
    "            logging.info(\" i have saved the raw dataset in artifact folder\")\n",
    "\n",
    "\n",
    "            logging.info(\" i have performed train test split\")\n",
    "            \n",
    "            train_data, test_data= train_test_split(data,test_size=0.25)\n",
    "            logging.info(\" train test split completed\")\n",
    "\n",
    "            train_data.to_csv(self.ingestion_config.train_data_path, index=False)\n",
    "            test_data.to_csv(self.ingestion_config.test_data_path, index=False)\n",
    "            \n",
    "            logging.info(\"data ingestion part completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(\"exception during data ingestion steps\")\n",
    "            raise customexception(e,sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.DimondPricePridiction.exception import customexception\n",
    "from src.DimondPricePridiction.logger import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder,StandardScaler\n",
    "from src.DimondPricePridiction.utils.utils import save_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path=os.path.join('artifacts','preprocessor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_transformation_config= DataTransformationConfig()\n",
    "\n",
    "    def get_data_transformation(self):\n",
    "        try:\n",
    "            \n",
    "            logging.info(\"Data transformation initiated\")\n",
    "\n",
    "            # Define which column should be ordinal encoded and which should be scaled\n",
    "            categorical_cols= ['cut','clarity','color']\n",
    "            numerical_cols= ['carat','depth','table','x','y','z']\n",
    "\n",
    "            # Define the custom ranking for each ordinal variable\n",
    "            cut_categories= ['Fair','Good','Very Good','Premium','Ideal']\n",
    "            color_categories= ['D','E','F','G','H','I','J']\n",
    "            clarity_categories= ['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF']\n",
    "\n",
    "            logging.info(\"Pipeline initiated\")\n",
    "\n",
    "            #Numerical Pipeline\n",
    "            num_pipeline= Pipeline(\n",
    "                steps=   [\n",
    "                ('imputer',SimpleImputer(strategy='median')),\n",
    "                ('scaler',StandardScaler())\n",
    "            ])\n",
    "\n",
    "            #Categorical Pipeline\n",
    "            cat_pipeline= Pipeline(\n",
    "                steps=[\n",
    "                ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                ('ordinalencoder',OrdinalEncoder(categories=[cut_categories,color_categories,clarity_categories])),\n",
    "                ('scaler',StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            preprocessor= ColumnTransformer([\n",
    "                ('num_pipeline',num_pipeline,numerical_cols),\n",
    "                ('cat_pipeline',cat_pipeline,categorical_cols)\n",
    "            ])\n",
    "            return preprocessor\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"error initiate in data transformation\")\n",
    "            raise customexception(e,sys)\n",
    "        \n",
    "    def initialize_data_transformation(self, train_path, test_path):\n",
    "        try:\n",
    "            train_df= pd.read_csv(train_path)\n",
    "            test_df= pd.read_csv(test_path)\n",
    "\n",
    "            logging.info(\"read train and test data complete\")\n",
    "            logging.info(f'Train data head : \\n{train_df.head().to_string()}')\n",
    "            logging.info(f'Test data head : \\n{test_df.head().to_string()}')\n",
    "            \n",
    "            preprocessing_obj= self.get_data_transformation()\n",
    "            \n",
    "            target_column_name= 'price'\n",
    "            drop_columns= [target_column_name,'id']\n",
    "\n",
    "            input_feature_train_df= train_df.drop(columns=drop_columns, axis= 1)\n",
    "            target_feature_train_df= train_df[target_column_name]\n",
    "\n",
    "            input_feature_test_df= test_df.drop(columns=drop_columns, axis= 1)\n",
    "            target_feature_test_df= test_df[target_column_name]\n",
    "\n",
    "            input_feature_train_arr= preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr= preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "            \n",
    "            logging.info(\"Applying preprocessing on train and test datasets\")\n",
    "\n",
    "            save_object(\n",
    "                file_path= self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj= preprocessing_obj\n",
    "            )\n",
    "            \n",
    "            return(\n",
    "                input_feature_train_arr,\n",
    "                input_feature_test_arr\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception initiate in data ingestion\")\n",
    "            raise customexception(e,sys)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "        self.data_transformation_config= DataTransformationConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_transformation(self):\n",
    "        try:\n",
    "            \n",
    "            logging.info(\"Data transformation initiated\")\n",
    "\n",
    "            # Define which column should be ordinal encoded and which should be scaled\n",
    "            categorical_cols= ['cut','clarity','color']\n",
    "            numerical_cols= ['carat','depth','table','x','y','z']\n",
    "\n",
    "            # Define the custom ranking for each ordinal variable\n",
    "            cut_categories= ['Fair','Good','Very Good','Premium','Ideal']\n",
    "            color_categories= ['D','E','F','G','H','I','J']\n",
    "            clarity_categories= ['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF']\n",
    "\n",
    "            logging.info(\"Pipeline initiated\")\n",
    "\n",
    "            #Numerical Pipeline\n",
    "            num_pipeline= Pipeline(\n",
    "                steps=   [\n",
    "                ('imputer',SimpleImputer(strategy='median')),\n",
    "                ('scaler',StandardScaler())\n",
    "            ])\n",
    "\n",
    "            #Categorical Pipeline\n",
    "            cat_pipeline= Pipeline(\n",
    "                steps=[\n",
    "                ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                ('ordinalencoder',OrdinalEncoder(categories=[cut_categories,color_categories,clarity_categories])),\n",
    "                ('scaler',StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            preprocessor= ColumnTransformer([\n",
    "                ('num_pipeline',num_pipeline,numerical_cols),\n",
    "                ('cat_pipeline',cat_pipeline,categorical_cols)\n",
    "            ])\n",
    "            return preprocessor\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"error initiate in data transformation\")\n",
    "            raise customexception(e,sys)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data_transformation(self, train_path, test_path):\n",
    "        try:\n",
    "            train_df= pd.read_csv(train_path)\n",
    "            test_df= pd.read_csv(test_path)\n",
    "\n",
    "            logging.info(\"read train and test data complete\")\n",
    "            logging.info(f'Train data head : \\n{train_df.head().to_string()}')\n",
    "            logging.info(f'Test data head : \\n{test_df.head().to_string()}')\n",
    "            \n",
    "            preprocessing_obj= self.get_data_transformation()\n",
    "            \n",
    "            target_column_name= 'price'\n",
    "            drop_columns= [target_column_name,'id']\n",
    "\n",
    "            input_feature_train_df= train_df.drop(columns=drop_columns, axis= 1)\n",
    "            target_feature_train_df= train_df[target_column_name]\n",
    "\n",
    "            input_feature_test_df= test_df.drop(columns=drop_columns, axis= 1)\n",
    "            target_feature_test_df= test_df[target_column_name]\n",
    "\n",
    "            input_feature_train_arr= preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr= preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "            \n",
    "            logging.info(\"Applying preprocessing on train and test datasets\")\n",
    "\n",
    "            save_object(\n",
    "                file_path= self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj= preprocessing_obj\n",
    "            )\n",
    "            \n",
    "            return(\n",
    "                input_feature_train_arr,\n",
    "                input_feature_test_arr\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception initiate in data ingestion\")\n",
    "            raise customexception(e,sys)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
